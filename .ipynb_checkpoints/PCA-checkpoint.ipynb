{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16de590-c335-4880-ae49-6c8ae11d9ee5",
   "metadata": {},
   "source": [
    "# Part 2: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a934c481-8c97-4cad-8534-7dfed66bd03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.image import iter_img\n",
    "from nilearn.plotting import plot_stat_map, show\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "\n",
    "from nilearn import datasets\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from scipy.stats import zscore\n",
    "from scipy.io import loadmat\n",
    "from IPython.display import Image\n",
    "\n",
    "save_results = 'results/'\n",
    "if not os.path.exists(save_results):\n",
    "    os.makedirs(save_results)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90eb967-3267-4994-af91-2206bb442cdf",
   "metadata": {},
   "source": [
    "## Loading and choosing the right data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a0a02a0-02d1-45db-b989-90ad02f380c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the next cell after making only one notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a1b99-6cd2-4c3a-999c-e865a26f3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'ds000171'\n",
    "subject = 'control01' \n",
    "\n",
    "sample_path = \"/home/jovyan/Data/dataset\"\n",
    "bids_root = op.join(sample_path, dataset_id)\n",
    "deriv_root = op.join(bids_root, 'derivatives')\n",
    "preproc_root = op.join(bids_root, 'derivatives','preprocessed_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e643feb9-2d00-4e21-89ee-7d18595253b8",
   "metadata": {},
   "source": [
    "Before applying the PCA to the fMRI data, we need to preprocessit. However, the preprocessing steps have already been done above. Therefore, we can reuse the already obtained preprocessed runs. We decided to keep the smoothing step in order to improve the signal to noise ratio of the data that the PCA will be run on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa0b08a-ae1f-4bbf-81fb-1eee93362f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = op.join(preproc_root, 'sub-control01', 'func','concatenated_bold_moco_smoothed-6mm.nii.gz')\n",
    "img = nib.load(op.join(preproc_root, 'sub-control01', 'anat','coregistered_anatomical.nii.gz'))\n",
    "affine = img.affine\n",
    "data = np.asanyarray(img.dataobj)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b2f58c-bdf5-45c3-ab1a-be0d1ead0652",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749ac24d-a438-41c1-8051-a7abbce38b26",
   "metadata": {},
   "source": [
    "Since the PCA is meant to track the evolution of the data over time, the samples are timepoints and the voxels are seen as features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5054db-3370-45e7-8864-0ae9e69f6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_shape = data.shape[0:3]\n",
    "n_vols = data.shape[3]\n",
    "\n",
    "num_zero_values_after = np.sum(data == 0)\n",
    "print(f\"Number of zero values after standardization: {num_zero_values_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b808f13e-e9de-41e2-9185-69f50e3c8be2",
   "metadata": {},
   "source": [
    "As we can see from the output of the previous cell, there is a great number of voxels with a null value. They correspond to the background, and were added at the end of the preprocessing step by applying a mask. In order to continue, we need to get rid of them since they are irrelevant for the PCA: they contain no data influenced by the experiment at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c319283-ddd6-4d0c-bd70-15acae60aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing background\n",
    "slice_non_background = np.ones(vol_shape, dtype=bool) # intialize a mask to keep track of non-background voxels\n",
    "samples = data[slice_non_background, :]\n",
    "print('original shape:', samples.shape)\n",
    "\n",
    "for vol in range(n_vols):\n",
    "    slice_non_background &= (data[:, :, :, vol] != 0) # use AND with every voxel in every volume to update mask\n",
    "\n",
    "samples = data[slice_non_background, :].T ## shape should be (n_timepoints, n_voxels) for PCA\n",
    "print('shape after removing background', samples.shape)\n",
    "\n",
    "if np.any(samples == 0):\n",
    "    print(\"Warning: There are still zero values in the samples array!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f3065-6864-4181-ba74-0e13d73414b6",
   "metadata": {},
   "source": [
    "Removing the spatial mean across timepoints from each timepoints. Calculates the mean spatial pattern and perform the substraction operation.\n",
    "# Why is the row mean not used (Does the substracion give the same thing without the row copying) ? Is the spatial mean correct ? Because the row is the same as in the series, but our dimentions are reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ec729-c9fa-467b-8835-245e98657eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean across columns\n",
    "spatial_means = np.mean(samples, axis=1, keepdims=True) # shape (n_timepoints, 1)\n",
    "\n",
    "# Subtract the means for each row, put the result into X\n",
    "X = samples - spatial_means\n",
    "\n",
    "# Verify that the spatial mean behaves as expected after substraction\n",
    "# Calculate the mean across time for each voxel to chek if its close to zero \n",
    "verification_means = np.mean(X, axis=1)\n",
    "\n",
    "# Print verification result\n",
    "print(\"Verification (should be close to zero): \\n\", np.round(verification_means, decimals=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78a6c4-6166-4e07-96a3-148916788b19",
   "metadata": {},
   "source": [
    "As we can see from the output of the previous cell, the values of the row-wise mean of the X matrix are close to zero. This means that our centering worked. Indeed, by subtracting the mean for each voxel across all time points, we make the time series of each voxel have an average value of zero, which is what we see here. What is left is to check weather the X matrix has the right shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c906235-c9ef-4eed-a740-0cfb3865f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f4d79e-5a1c-4c48-acc1-3f3e46a49bf3",
   "metadata": {},
   "source": [
    "Which is the case: is has the same shape as the samples matrix after the removal of the zero voxels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f72d1b-63e8-459d-a52b-f2c1bef3b601",
   "metadata": {},
   "source": [
    "# Components extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b1a40-568a-4288-a6df-cf33011151a9",
   "metadata": {},
   "source": [
    "Now we can move on to the extraction of components. In the next cell we run the PCA. There is no need to run it on the transpose of the X matrix, since it already has the right dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef03c3a1-7c05-46b3-b757-7818e8a1f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "nb_components = 10 # Arbitrary number\n",
    "\n",
    "pca = PCA(n_components=nb_components)\n",
    "\n",
    "# Fit the PCA model to the data (where samples are voxels and features are time points)\n",
    "pca.fit(X)\n",
    "\n",
    "# Retrieve the principal components\n",
    "components = pca.components_  # Shape (nb_components, n_timepoints)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Explained variance by each component:\", explained_variance)\n",
    "print(\"Components shape:\", components.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefe75f5-90c8-4ec4-8eec-20374cefecde",
   "metadata": {},
   "source": [
    "Now we must chose a number of components that will explain a sufficient amount of variance. We know that the bogger an eigenvalue is, the more variance it explains. Therefore, we will sort them and plot sum of the explained variance as a funcion of the first considered eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef9343e-2bf7-4a7f-906b-03f0394ca3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "ratios = pca.explained_variance_ratio_\n",
    "cumulative_ratios = np.cumsum(pca.explained_variance_ratio_)\n",
    "nb_clusters = 3\n",
    "\n",
    "ax[0].plot(np.arange(1, len(ratios)+1), ratios, label='explained variance ratios', c='k')\n",
    "ax[0].scatter([nb_clusters], [ratios[nb_clusters-1]], [150], marker='x', color='r', label='cutoff')\n",
    "ax[1].plot(np.arange(1, len(cumulative_ratios)+1), cumulative_ratios, label='cumulative explained variance', c='k')\n",
    "ax[1].hlines(y=cumulative_ratios[nb_clusters-1], xmin=0, xmax=len(ratios)-1, linestyle='--', color='r', label='cutoff')\n",
    "\n",
    "for k in range(2):\n",
    "    ax[k].set_xlabel('Components #', size=15)\n",
    "    ax[k].legend(prop={'size':15})\n",
    "    ax[k].tick_params(axis='both', which='major', labelsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e18d19-b722-42cd-b80a-5e8fe69673d4",
   "metadata": {},
   "source": [
    "From these graphs, we can see that keeping the first three components explains almost 75% of the total variance. Now, from these three components, we will reconstruct spatial volumes and overlay them onto anatomical data to visualise the parts of the brain that are the most influenced by the notes and songs heard during the fMRI runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c85cf8a-3af2-4b4e-b745-3f001263efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize an empty list to store each spatial component as a 3D volume\n",
    "pca_clusters = []\n",
    "\n",
    "# For each component, reshape it back to the original spatial shape using the mask\n",
    "for spatial_component in components:\n",
    "    # Step 1: Initialize a 3D volume with the original shape, filled with zeros or NaN\n",
    "    component_volume = np.zeros(vol_shape)  # Or use np.full(vol_shape, np.nan) for NaNs\n",
    "\n",
    "    # Step 2: Place the values from the component back into the spatial volume\n",
    "    component_volume[slice_non_background] = spatial_component\n",
    "\n",
    "    # Step 3: Append the reshaped 3D volume to the list\n",
    "    pca_clusters.append(component_volume)\n",
    "\n",
    "# `pca_clusters` now contains reconstructed spatial volumes for each PCA component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e998d-a53b-4226-839f-bbdca5f9d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_stat_map\n",
    "from nilearn.image import mean_img\n",
    "mean_img_ = mean_img(img)\n",
    "\n",
    "for visual_idx in range(nb_clusters):\n",
    "    plot_stat_map(nib.Nifti1Image(pca_clusters[visual_idx], affine), bg_img=mean_img_, threshold=0.01,\n",
    "                   cut_coords=[-40, -30, -20, -10, 0, 10, 20, 30, 40], black_bg=True, display_mode = \"z\",\n",
    "                  title=f'PCA Cluster {visual_idx}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca619cd-9af6-460a-a2df-c7b894e10b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
